# 信息检索系统设计与实现报告

**项目名称：** 基于GME-Qwen2-VL的交互式多模态信息检索系统

**作者：** 唐梓楠 2022211404、连琎宇 2022211394

**日期：** 2025-06-13

---

## 摘要

本项目设计并实现了一个先进的交互式多模态信息检索系统。传统的信息检索系统多局限于文本处理，无法有效应对日益增长的图像、图文等混合数据类型的检索需求。为解决此问题，本系统创新性地采用了在 A100 服务器上部署阿里巴巴开源的70亿参数大型视觉语言模型 GME-Qwen2-VL 作为核心引擎，实现了对文本、图像以及图文内容的统一语义理解和嵌入。系统后端使用 ChromaDB 向量数据库进行高效的相似性检索，前端通过 Gradio 构建了直观的可视化操作界面。用户不仅可以进行多模态的交叉检索，还可以对检索结果的准确性进行实时反馈，为系统的迭代优化提供了数据支持。测试结果表明，本系统能够准确、快速地响应用户的多模态查询，在处理复杂语义和跨模态检索任务上表现出色。

## 引言

### 项目背景
随着互联网信息的爆炸式增长，信息的载体早已不局限于单一的文本形式，图片、视频、音频等多媒体数据与文本交织在一起，形成了复杂的多模态信息环境。用户常常需要在这种混合数据中查找信息，例如"寻找一张关于'可持续农业'的图片并附带相关说明"。传统的基于关键词匹配或单一模态的检索技术，难以准确理解用户的跨模态检索意图，也无法有效衡量不同模态数据间的相关性。因此，研究和实现能够理解并检索多模态内容的信息检索系统，具有重要的理论意义和广阔的应用前景。

### 项目目标
本项目的主要目标是构建一个满足现代化信息检索需求的原型系统，具体包括：
- **实现多模态数据处理**：系统应能处理和存储包括纯文本、纯图像以及图文混合在内的多种数据类型。
- **构建高级检索模型**：摒弃传统的倒排索引和词袋模型，采用先进的大型视觉语言模型（LVLM）将不同模态的信息映射到统一的高维向量空间。
- **支持多模态查询**：允许用户使用文本、图像或图文结合的方式进行查询，并返回最相关的多模态文档。
- **提供友好交互界面**：设计一个简洁直观的图形用户界面（GUI），方便用户进行数据的增添和检索操作。
- **建立结果评估机制**：集成用户反馈功能，对检索结果的准确率进行量化评估，满足课程的扩展要求。

## 系统设计

### 系统总体架构
本系统采用模块化设计，主要由用户界面层、应用逻辑层和数据存储层三部分构成，各层之间分工明确，耦合度低。

```mermaid
graph TD;
    subgraph 用户界面层 (UI Layer)
        A[Gradio Web UI]
    end

    subgraph 应用逻辑层 (Application Logic)
        B[应用服务器 app.py]
        C[检索器 Retriever]
        D[核心模型 GME-Qwen2-VL]
    end

    subgraph 数据存储层 (Data Storage)
        E[向量数据库 ChromaDB]
        F[原始文件存储 data/]
    end

    A -- HTTP Request (查询/添加) --> B;
    B -- 调用检索/嵌入方法 --> C;
    C -- 调用模型生成嵌入 --> D;
    C -- 存储/查询向量 --> E;
    B -- 存储原始图片 --> F;
    E -- 返回检索结果 --> C;
    C -- 返回处理结果 --> B;
    B -- HTTP Response (结果/状态) --> A;
```
* **用户界面层 (UI Layer)**：作为用户与系统的交互入口，采用 `Gradio` 库构建。它提供了数据上传、文本输入、参数调整等控件，并将用户的请求发送到应用逻辑层，同时负责将返回的结果以可视化的方式呈现给用户。
* **应用逻辑层 (Application Logic)**：系统的核心。`app.py`作为Web应用的主体，负责接收和响应前端请求。`Retriever`模块是检索功能的核心实现，它封装了对`GME-Qwen2-VL`模型的调用，能够为不同模态的数据生成高质量的"融合"嵌入向量。
* **数据存储层 (Data Storage)**：负责数据的持久化。`ChromaDB`向量数据库用于存储所有内容的向量嵌入及其元数据（如标题、URL、类型等），并提供高效的相似度检索。原始的图片文件则保存在本地的`data/`目录中。

### 数据获取与处理
系统支持用户通过界面手动添加数据。数据源可以是本地的文本文档、图片，或是从网络爬取后整理的包含URL、标题、内容等信息的结构化数据。对于不同类型的数据，系统会进行相应的预处理：
- **文本**：直接输入或读取的文本内容。
- **图像**：用户上传的图像文件，系统会保存副本并记录其路径。
- **图文**：同时包含文本描述和关联的图像，系统将两者关联存储。

### 核心检索模型
本项目的最大创新点是采用了 **GME-Qwen2-VL** 模型作为统一的特征提取器。该模型是一个拥有70亿参数的大型视觉语言模型，它通过在海量图文数据上的预训练，学习到了将视觉信息和语言信息映射到同一个语义空间的能力。

与传统方法（如分别为图像和文本提取特征再进行拼接）不同，`GME-Qwen2-VL` 能够生成真正的**多模态融合嵌入（Fused Embedding）**。当处理图文对时，模型内部的注意力机制能够捕捉图像内容与文本描述之间的深层交互关系，生成一个同时蕴含两者语义的单一向量。这一特性使得跨模态检索（如以图搜文，以文搜图）的准确性得到极大提升。

此外，在生成查询向量时，系统利用了**指令嵌入（Instruction Embedding）**技术。通过向模型提供明确的任务指令（如 "Find a document that matches the given query."），可以使生成的查询向量更具指向性，从而在检索时与文档向量实现更精准的匹配。

### 向量数据库
本系统选用 `ChromaDB` 作为向量数据库。它是一个专为AI应用设计的开源数据库，具备以下优点：
- **高效检索**：底层使用HNSW等近似最近邻（ANN）算法，能够在上百万甚至上亿的向量中实现毫秒级的快速查询。
- **元数据存储与过滤**：支持在存储向量的同时附加结构化的元数据，并可在查询时基于元数据进行过滤，灵活性高。
- **持久化存储**：支持将数据集合持久化到磁盘，保证了数据的安全性和可复用性。

在本项目中，每条数据（无论其模态）的嵌入向量及其元数据（类型、标题、内容路径、URL、日期）都被存储在 `ChromaDB` 的一个集合中。当用户发起查询时，系统首先将查询内容转换为查询向量，然后利用 `ChromaDB` 的 `query` 方法计算查询向量与数据库中所有向量的相似度（如余弦相似度），最终返回最相似的 `top_k` 个结果。

## 系统实现

### 主要功能模块
主要功能由 `app.py`, `retriever.py`, 和 `database.py` 三个文件协作完成。
- `database.py`：封装了对 `ChromaDB` 的所有操作，包括客户端初始化、创建或加载集合、添加数据（`add`方法）和执行查询（`query`方法）。
- `retriever.py`：负责所有与AI模型相关的逻辑。它在初始化时加载 `GME-Qwen2-VL` 模型，并提供 `get_text_embedding`, `get_image_embedding`, `get_image_text_embedding` 等接口。`search` 方法是其核心，它根据用户输入判断查询类型，调用相应接口生成查询向量，并向 `database` 模块发起查询请求。
- `app.py`：使用 `Gradio` 搭建Web界面，定义了"数据管理"和"信息检索"两个Tab。它处理所有用户交互事件，调用 `Retriever` 和 `Database` 实例来完成后台任务，并负责动态更新前端显示的内容，包括检索结果和准确率统计。

### 关键技术
- **核心模型**: `Alibaba-NLP/gme-Qwen2-VL-7B-Instruct`
- **深度学习框架**: `PyTorch`, `transformers`, `accelerate`
- **向量数据库**: `ChromaDB`
- **Web UI框架**: `Gradio`
- **图像处理**: `Pillow (PIL)`
- **数值计算**: `Numpy`

### 用户交互界面
系统界面（通过 `app.py` 启动）分为两个主要功能区：

**数据管理 (Manage Data)**
在此页面，用户可以选择要添加的数据类型（文本、图像、图文），并填写标题、内容、URL等信息，上传图片。点击"Add Item"按钮后，系统会执行嵌入和存储流程，并在右侧显示操作状态。

**检索 (Search)**
在此页面，用户可以在文本框中输入查询语句，或上传一张查询图片（或两者都提供）。通过滑动条可以设置返回结果的数量（Top K）。点击"Search"按钮后，右侧会以HTML格式展示图文并茂的检索结果列表，包含相关度分数、标题、内容详情、URL和日期。

## 系统测试与评估

### 功能测试
对系统的各项功能进行了全面的测试，结果表明所有功能均可正常运行。
- **数据添加**：成功测试了纯文本、纯图像和图文混合三种类型数据的添加，数据均能被正确地转换成向量并存入数据库。
- **文本检索**：输入文本查询，能够准确返回相关的文本、图文或图像内容。
- **图像检索**：上传一张图片，能够检索出内容或风格相似的图像。
- **图文联合检索**：同时提供文本和图片，能够返回与两者语义最匹配的结果，展现了模型强大的多模态理解能力。

### 检索结果准确率人工评价
为量化评估系统的检索性能，本项目特别设计并实现了**检索结果准确率人工评价功能**，这直接满足了课程的扩展要求。在"Search"界面的检索结果下方，设置了"👍 Accurate"和"👎 Inaccurate"两个反馈按钮。
- **工作流程**：用户在完成一次检索后，可以根据自己对结果的主观判断，点击相应的按钮。
- **数据记录**：每次点击都会触发 `record_feedback` 函数，该函数会更新并保存一个名为 `feedback_scores.json` 的文件，其中记录了总的评价次数（`total`）和被认为是准确的次数（`accurate`）。
- **实时显示**：界面上会实时计算并显示当前的准确率，格式为 "**Accuracy:** XX.X% (accurate / total votes)"。

这个闭环的反馈系统不仅为衡量模型性能提供了直观的量化指标，也为未来模型的微调和优化收集了宝贵的标注数据。

## 创新点与思考

### 基于大型视觉语言模型的多模态检索
本项目最大的创新之处在于没有采用传统的、分模块的检索思路，而是直接应用了前沿的、端到端的大型视觉语言模型 `GME-Qwen2-VL`。这带来了几个核心优势：
1.  **统一的语义空间**：从根本上解决了多模态信息难以对齐和比较的问题。
2.  **强大的泛化能力**：预训练模型见过了海量数据，使其对各种概念和语境都有很好的理解，即使是未见过的查询内容也能处理。
3.  **真正的融合理解**：通过模型内部的融合机制，系统能够理解图文之间复杂的交互关系，而不仅仅是内容的简单叠加。

这次尝试证明了大型AI模型在信息检索领域的巨大潜力，它将检索系统的核心从"关键词匹配"提升到了"语义理解"的层面。

### 对环境和社会可持续发展影响的考量
信息技术的快速发展同样带来了能源消耗的挑战。本项目在设计和实现中考虑了可持续性问题：
1.  **计算资源优化**：通过使用 `accelerate` 库和 `float16` 半精度加载模型，在保证模型性能的同时，显著降低了显存占用和计算能耗。相比于使用 `float32` 全精度，推理阶段的能耗可降低近一半。
2.  **高效的检索算法**：采用ANN算法的向量数据库，避免了在检索时对全库数据进行暴力比较，极大地提升了查询效率，减少了不必要的计算开销。这在使用大规模数据时，节能效果尤为明显。
3.  **知识传播应用**：本系统可以被应用于特定领域，如搭建一个关于"环境保护"、"可持续发展"或"生物多样性"的多媒体知识库。用户可以通过图片或文字快速检索相关知识，有助于相关信息的传播和普及，产生积极的社会影响。

### 总结与展望
本项目成功构建了一个功能完善、技术先进的交互式多模态信息检索系统。通过集成 `GME-Qwen2-VL` 模型和 `ChromaDB` 数据库，系统在处理和检索多模态数据方面展现了优异的性能。同时，项目的开发过程也为我们深入理解前沿AI技术在信息检索领域的应用提供了宝贵的实践经验。

**未来展望**：
- **模型微调（Fine-tuning）**：利用用户反馈收集的数据，可以在特定领域对 `GME-Qwen2-VL` 模型进行微调，以进一步提升检索的精准度。
- **支持更多模态**：将系统扩展以支持视频、音频等更多的数据类型，构建一个更加全面的多媒体检索平台。
- **部署与优化**：对模型进行量化或蒸馏，以减小其体积和推理延迟，使其更易于部署到服务器甚至边缘设备上。

## 参考文献

- [1] GME-Qwen2-VL Model Card. Alibaba-NLP. https://huggingface.co/Alibaba-NLP/gme-Qwen2-VL-7B-Instruct
- [1] Zhang X, Zhang Y, Xie W, et al. GME: Improving Universal Multimodal Retrieval by Multimodal LLMs[J]. arXiv preprint arXiv:2412.16855, 2024.
- [2] ChromaDB Documentation. https://docs.trychroma.com/
- [3] Gradio Documentation. https://www.gradio.app/docs/
